\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{stfloats}
\usepackage[pagebackref=true,colorlinks,linkcolor=red,citecolor=green,breaklinks=true,bookmarks=false]{hyperref}
\cvprfinalcopy
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\title{Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset}
\author{Cheng Guan\\\\
August 24, 2018}
\begin{document}
\maketitle
\begin{abstract}
The paucity of videos in current action classification
datasets (UCF-101 and HMDB-51) has made it difficult
to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art 
architectures in light of the new Kinetics Human Action Video
dataset. Kinetics has two orders of magnitude more data,
with 400 human action classes and over 400 clips per
class, and is collected from realistic, challenging YouTube
videos.
\par
The authors also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image 
classification ConvNets are expanded into 3D, making it possible
to learn seamless spatio-temporal feature extractors from
video while leveraging successful ImageNet architecture
designs and even their parameters.
\end{abstract}
\section{Introduction}
One of the unexpected benefits of the ImageNet challenge has been the discovery that deep architectures trained
on the 1000 images of 1000 categories, can be used for other
tasks and in other domains. One of the early examples of
this was using the fc7 features from a network trained on
ImageNet for the PASCAL VOC classification and detection 
challenge \cite{girshick2014rich,oquab2014learning}. Furthermore, improvements in the deep architecture, changing from AlexNet to VGG-16, immediately fed through to commensurate improvements in
the PASCAL VOC performance \cite{ren2015faster}. Since then, there have
been numerous examples of ImageNet trained architectures
warm starting or sufficing entirely for other tasks, e.g. segmentation, depth prediction, pose estimation, action classification.
\par
In this paper The authors aim to provide an answer to this question
using the new Kinetics Human Action Video Dataset \cite{kay2017kinetics},
which is two orders of magnitude larger than previous
datasets, HMDB-51  and UCF-101 . Kinetics has 
400 human action classes with more than 400 examples for
each class, each from a unique YouTube video.
\section{Action Classification Architectures}
A graphical overview of the five types of architectures
they evaluate is shown in Fig.\ref{fig1}
\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{2.png}
	\label{fig1}
	\caption{Video architectures considered in this paper. \textbf{K} stands for the total number of frames in a video, whereas \textbf{N} stands for a subset of
		neighboring frames of the video.}
\end{figure*}
\subsection{The Old I: ConvNet+LSTM}
In theory, a more satisfying approach is to add a recurrent layer to the model \cite{donahue2015long}, such as an LSTM, which can
encode state, and capture temporal ordering and long range
dependencies. The authors position an LSTM layer with batch normalization after the
last average pooling layer of Inception-V1, with 512 hidden units. \textbf{A} fully connected layer is added on top for the
classifier.
\subsection{The Old II: 3D ConvNets}
D ConvNets seem like a natural approach to video mod-
eling, and are just like standard convolutional networks, but
with spatio-temporal filters. They have been explored several times, previously \cite{ji20133d,taylor2010convolutional}. They have a very im-
portant characteristic: they directly create hierarchical representations of spatio-temporal data. One issue with these
models is that they have many more parameters than 2D
ConvNets because of the additional kernel dimension, and this makes them harder to train.
\subsection{The Old III: Two-Stream Networks}
A different, very practical approach, introduced by Simonyan and Zisserman \cite{simonyan2014two}, models short temporal snapshots of videos by averaging the predictions from a single
RGB frame and a stack of 10 externally computed optical flow frames, after passing them through two replicas of an
ImageNet pre-trained ConvNet. The flow stream has an
adapted input convolutional layer with twice as many input
channels as flow frames (because flow has two channels,
horizontal and vertical), and at test time multiple snapshots
are sampled from the video and the action prediction is averaged.
\subsection{The New: Two-Stream Inflated 3D ConvNets}
With this architecture, The authors show how 3D ConvNets can
benefit from ImageNet 2D ConvNet designs and, 
optionally, from their learned parameters. They also adopt a two-stream configuration here. While 3D ConvNets can directly learn about temporal
patterns from an RGB stream, their performance can still be
greatly improved by including an optical-flow stream.
\par
\noindent \textbf{Inflating 2D ConvNets into 3D.} A number of very 
successful image classification architectures have been developed
over the years, in part through painstaking trial and error.
Instead of repeating the process for spatio-temporal models
the authors propose to simply convert successful image (2D) 
classification models into 3D ConvNets. This can be done by
starting with a 2D architecture, and inflating all the filters
and pooling kernels – endowing them with an additional
temporal dimension.
\par
\noindent \textbf{Bootstrapping 3D filters from 2D Filters.} 
Besides the architecture, one may also want to bootstrap parameters from
the pre-trained ImageNet models. To do this, the authors observe
that an image can be converted into a (boring) video by
copying it repeatedly into a video sequence. The 3D models
can then be implicitly pre-trained on ImageNet, by satisfying 
what they call the boring-video fixed point: the pooled
activations on a boring video should be the same as on the
original single-image input. This can be achieved, thanks to
linearity, by repeating the weights of the 2D filters \textit{N} times
along the time dimension, and rescaling them by dividing
by \textit{N}.
\par
\noindent \textbf{Pacing receptive field growth in space, time and network depth.}
The boring video fixed-point leaves ample
freedom on how to inflate pooling operators along the time
dimension and on how to set convolutional/pooling tempo-
ral stride – these are the primary factors that shape the size
of feature receptive fields. Virtually all image models treat
the two spatial dimensions (horizontal and vertical) equally
– pooling kernels and strides are the same.
\par
\noindent \textbf{Two 3D Streams.}
While a 3D ConvNet should be able to
learn motion features from RGB inputs directly, it still performs pure feedforward computation, whereas optical flow
algorithms are in some sense recurrent (e.g. they perform iterative optimization for the flow fields).
\bibliography{I3D}
\bibliographystyle{ieee}
\end{document}