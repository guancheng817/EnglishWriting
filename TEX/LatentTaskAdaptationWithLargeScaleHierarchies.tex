\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx,float}
\usepackage{indentfirst}
\usepackage{balance}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green
            ]{hyperref}
\usepackage{geometry}
\usepackage{cvpr}
\ifcvprfinal\pagestyle{empty}\fi
\cvprfinalcopy
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}
\title{\textbf{Latent Task Adaptation with Large-scale Hierarchies}}
\author{Cheng Guan}

\begin{document}
\maketitle
\begin{abstract}
Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on ImageNet can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and suboptimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a linear time probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model,and an open-source tool box to train classifiers distributedly at a large scale. Empirical results based on the ImageNet data showed significant performance increase over several baseline algorithms
\end{abstract}

\section{Introduction}
Recent years have witnessed a growing interest in object classification tasks involving specific sets of object categories,such as fine-grained object classification \cite{Birdlets2011,dataset2011}and home object recognition in visual robotics. Existing methods in the literature generally describe algorithms that are trained and tested on exactly the same task, \emph{i.e.} we assume the training data and testing data share the same set of object labels. A dog breed classifier is trained and tested on dogs and a cat breed classifier done on cats,without the use of out-of-task images.
\par
However, two observations may render this ¡°one (multiclass) classifier per task¡± approach suboptimal. First, it¡¯s known that using images of related tasks is often beneficial to build a better model for the general visual world \cite{Selftaught2007}, which serves as a better regularization for the specific task as well. Second,object categories in the real world are often organized in,or at least well modeled by,an nested taxonomical hierarchy(\emph{e.g.} Figure ~\ref{fig1}),with classification task scorresponding to intermediate subtrees in this hierarchy, and recent efforts on the ImageNet challenge \cite{Signature2011,ImageNet2012} have leveraged the use of large-scale data to learn such information. While it is reasonable to train separate classifiers for specific tasks, this quickly becomes infeasible as there are a huge number of possible tasks - any subtree in the hierarchy may be a latent task requiring one to distinguish object categories under the subtree.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{1.png}\\
  \caption{Top: Visualization of specific object classification tasks of interest in daily life, which are often subtrees in a large scale object taxonomy, \emph{e.g.} the ImageNet hierarchy. Bottom: Adapting the ImageNet classifier allows us to perform accurate prediction(bold),while the original classifier prediction (in parentheses) suffers from a higher confusion.
}\label{fig1}
\end{figure}

\section{Related Work}
The problem of task adaptation is analogous to, but essentially distinctive from domain adaptation. While domain adaptation aims to model the perceptual difference of the training and testing images from the same labels,task adaptation focuses on modeling the conceptual difference: different label spaces during training and testing. Additionally,as one is often able to use large amounts of data during training, we assume that the testing tasks involve subsets of labels encountered during training time.
\par
Predicting the intermediate concept in a hierarchy with a set of examples has been discussed in psychology . These methods often make a simplified assumption that labels (leaf nodes in the hierarchy) are given for the input images. We believe our paper is the first to connect such psychological study with computer vision research by directly taking perceptual inputs, allowing one to perform generalization with images of unknown category.
\section{A Generative Model for Task Adaptation}
Formally, we define a classification task to be a subset of all the possible object labels that are semantically related (such as all breeds of dogs in ImageNet). During testing time,a number of query images are randomly sampled from the labels belonging to a task, and the learning algorithm needs to give predictions on these images. In this section we propose a probabilistic framework that models the generation of latent tasks and the test time query images.
\par
As stated in the previous section,we are interested in the scenario when the task is latent,\emph{i.e.}only implicitly specified by the query images. We introduce two key components for modeling the generative process of query images: a latent task space that defines possible tasks and their probability, and a procedure to sample query images given a speci?c latent task. Specifically, we propose the graphical model in the Figure ~\ref{fig2} which generates a set of \emph{N} query images when given \emph{T} possible tasks and \emph{K} object categories:
\begin{enumerate}
  \item Sample a latent task h from the task priors $p\left(h\right)$ with hyperparameter $\alpha$;
  \item For the \emph{N} query images:
      \begin{enumerate}[(a)]
      \item  Sample an object category $y_i$ from the conditional probability $P\left(y_i|h;\beta_h\right)$.
      \item  Sample a query image from category $y_i$ with $P\left(y_i|h;\theta_{y_i}\right)$.
     \end{enumerate}

\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.4]{2.png}\\
  \caption{Left: the generative model for the latent task and corresponding query images. Right: the prior probabilities of the latent tasks from psychological study, along the path leading to the synsets oriental poppy and can opener respectively, with darker color indicating higher probability}\label{fig2}
\end{figure}

{\small
\bibliographystyle{ieee}
\bibliography{Large}
}

\end{document}